<img width="350" height="300" alt="Project Screenshot" src="https://github.com/user-attachments/assets/472ce654-81c1-41e6-b420-e7c81b9d5587" />
<br>
<br>


Note: This work is an extension after the summer school with KAUST, original work can be seen through the following [link](https://github.com/Abdulrahman-Alfrihidi/Summer-project)
<br>
Precise video retrieval requires multi-modal correlations to handle unseen vocabulary and scenes, becoming more complex for lengthy videos where models must perform effectively without prior training on a specific dataset. We introduce a unified framework that combines a visual matching stream and an aural matching stream with a unique subtitles-based video segmentation approach. Additionally, the aural stream includes a complementary audio-based two-stage retrieval mechanism that enhances performance on long-duration videos. Considering the complex nature of retrieval from lengthy videos and its corresponding evaluation, we introduce a new retrieval evaluation method specifically designed for long-video retrieval to support further research. We conducted experiments on the YouCook2 benchmark, showing promising retrieval performance.


Initial demo for the archeticture i've recorded: [Video](https://www.youtube.com/watch?v=MPHTWzl813M) <br>
Paper Link: [Paper](https://arxiv.org/abs/2504.04572)
