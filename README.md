<img width="300" height="250" alt="Project Screenshot" src="https://github.com/user-attachments/assets/472ce654-81c1-41e6-b420-e7c81b9d5587" />




Precise video retrieval requires multi-modal correlations to handle unseen vocabulary and scenes, becoming more complex for lengthy videos where models must perform effectively without prior training on a specific dataset. We introduce a unified framework that combines a visual matching stream and an aural matching stream with a unique subtitles-based video segmentation approach. Additionally, the aural stream includes a complementary audio-based two-stage retrieval mechanism that enhances performance on long-duration videos. Considering the complex nature of retrieval from lengthy videos and its corresponding evaluation, we introduce a new retrieval evaluation method specifically designed for long-video retrieval to support further research. We conducted experiments on the YouCook2 benchmark, showing promising retrieval performance.


Initial demo for the archeticture i've recorded: [Video](https://www.youtube.com/watch?v=MPHTWzl813M)
